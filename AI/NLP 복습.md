불용어(Stop words) : '은/는/이/가','하다'처럼 딱히 존재감이 없는 애들 ㅠㅠ 신경쓰지 않아도 딱히 문제가 없다.  

ex)

```
nlp.Defaults.stop_words
```

이것도 디폴트야 ㅠㅠ --> 커스터마이징 할 수 있음..

존재감 없으니까 그것도 귀찮아서 통계로 자르면 '통계적 트리밍' 할 수 있음.

**어간 추출(Stemming)** : 에러가 떠서 구글링 해야 할때, 최대한 baby 처럼 핵심만 뽑아내야하는데 거기서 생후 1달차 처럼 말하다 말정도로 핵심. (그래서 빨리 찾을 수 있음) 

#### **표제어 추출(lemmatization :** 공식 document로 에러, 개념 찾기. 문서에 문서로 들어가서 찾아야하고 뇌가 더 돌아가지만 더 정확하게 찾을 수 있음. (ex tf.model 에서 에러가 났으면 model 들어가서 다 봐야함)

어느 백화점 직원이 본사에 손님들의 방문 데이터를 컴퓨터를 통해 보고해야한다. 어떻게 할까? 

**Bag-of-Words(BoW) : TF(Term Frequency):** 백화점 직원이 단순히 각 손님이 매장을 한달에 몇번 방문했는지 세서 표로 정리하는 것.  

#### **TF-IDF :** 각 손님이 와서 얼마를 썼는지도 빈도수에 곱해서 보고 하는 것. 

(👀주의 : 나만의 언어로 이해하기 위해 쓰는 것으로 올바른 언어, 사상과는 상당한 거리가 있음.)

**1\. 분포 가설(Distributed Representation) :** 끼리 끼리더라, 유유상종 기반으로 인간을 보는 것, 근데 출신 지역, 학력, 전공을 곁들인..

-   원핫 인코딩: 장점: 단어를 벡터화 하기 제일 쉬운 방법 / 단점: 단어와 단어 끼리의 유사도, 즉 쟤랑 쟤랑 얼마나 친한 사이인지를 모름.
-   임베딩(Embedding): 원핫 인코딩의 단점을 보완, \[embed: 끼워 넣다\] 그러니까 단어끼리의 친한 정도도 끼워 넣어 줘서 끼리 끼리를 더 잘 파악할 수 있게, 그래서 '끼워넣기\[임베딩\]'

아니..원핫 인코딩의 단점까지 보완해서 해결책을 '임베딩'을 끼워 넣어줬다니.. 너무 좋잖아..? 그러면 많은 방법들이 나왔겠지?  
가장 대표적인 건?  
**2\. Word2Vec**\-> 얘도 끼리끼리(분포가설) 전제

-    CBoW(Continuous Bag-of-Words) : 상황: 갑자기 친구가 남자 소개를 해준다고 한다.. 친구도 그 남자는 잘 모르고 주변 친구들만 안다고 한다.**"그럼 그 분 주변 친구들은 어떤 성격이셔?"** 즉,베프들의 상태를 보고 그 사람의 상태를 예측하는 것.
-    Skip-gram : 상황: 친구가 자기 친구를 소개시켜주고 싶다고 한다. 그때 생각한다. "아 xx가 착하고 예쁘고 똑똑하니까 그 친구도 그렇겠구나 빨리 만나보고 싶다!! ㅎㅎ" 즉, 중심 단어 정보 기반으로 주변 단어 정보 예측

이렇게 '끼리 끼리'로 사람을 예측해도...결국\_나는\_계속 새로운 사람을 만나야하는 '링겐'이잖아?ㅠㅠ  
\--> **OOV(Out of Vocabulary)** 문제  
세상 모든 사람이 친구의 친구면 좋겠지만....(또르륵) 그래서 그 해결책으로 나타난게 바로바로

**3\. fastText** \= 원핫인코딩+임베딩(관계성)+**임베딩(철자(Character) 기반)**\_빠르게 예측하고 싶었나봐 (fast)  
쪼개쪼개서 들어가 / EX) eating= ea/eat/ati/tin/ing/ng  
하..정보가 없잖아..페북 친구 목록에 들어가..함께 아는 친구..? 어 친구는 아닌데 같은 페친이 있네..? 그 사람 정보를 들어가서봐... 아니면 함께 아는 친구는 없는데 이 사람 친구들 중에 나랑 페친이 되어있는 사람들이 있네..? 그 사람들 타임라인을 또 봐.. (하지만 철자 단위는 아니니까 타고 타고 쪼개어서 들어간다는 의미로만 파악)

**다시한번 느끼지만,,,DISCUSSION 효과적이야..**
