[ì£¼ì˜: ë°°ìš°ëŠ” ì¤‘, ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš©ì´ ì–´ë ¤..ìš´ ê´€ê³„ë¡œ í•™ìŠµ ëª©í‘œì— ë§ì¶˜ ë‚˜ì˜ ì‘ê³  ê·€ì—¬ìš´ íœ˜ëšœë£¨ ë§ˆëšœë£¨ ì´í•´]

-í†µê³„ì  ì–¸ì–´ ëª¨ë¸ (ì „í†µì ) 
íšŸìˆ˜ê¸°ë°˜ (í™•ë¥ ) -> OOV -> Sparsity (í¬ì†Œì„±)ë¬¸ì œ  
- So, ì–´ì œ ë°°ìš´ [ì„ë² ë”© ë²¡í„°]ë¥¼ ì¨ì„œ Word2vec, fastTextê°™ì€ ìœ ì‚¬ì„±ì„ ë¼ì›Œë„£ì–´ ì¤Œ.

- This is ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ -> í¬ì†Œì„± í•´ì†Œ  
---> ì´ ì–¸ì–´ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê²ƒ
### **---> :ìˆœí™˜ ì‹ ê²½ë§ (RNN)** 

### ì–˜ íŠ¹ 
- Sequential í•œ ë°ì´í„°ë¥¼ ì²˜ë¦¬ 
- ìˆœí™˜(Recurrent) : ->ì€ë‹‰-> ì…ë ¥->ì€ë‹‰ -> 
   (ì•½ê°„ ê¸‰ì‹ì‹¤ ì¤„ì„œëŠ”ë° ì˜¤ëŠ˜ ìš”êµ¬ë¥´íŠ¸ ë‚˜ì™€ì„œ ê³„ì† ì• ë“¤ì´ ë‹¤ì‹œ ëª¨ë¥¸ì²™í•˜ê³  ë˜ ì¤„ì„œëŠ” ëŠë‚Œ) 

- ì´ë¡ ì ìœ¼ë¡œëŠ” ë¬´í•œëŒ€ë¡œ ì¤„ì„ ëŠ˜ë ¤ì„œ ê¸‰ì‹ì„ ì„¤ìˆ˜ìˆê¸´ í•˜ì§€.. (ì–´ë–¤ ê¸¸ì´ì˜ sequential ë°ì´í„°ë„ ì´ë¡ ì ìœ¼ë¡œëŠ” ì²˜ë¦¬ ê°€ëŠ¥) 
- ê·¼ë° ë¬´ì¡°ê±´ í•œì¤„ ì„œê¸°ì„ ë‘ì¤„ ì„¸ì¤„ XX (ë³‘ë ¬í™” ë¶ˆê°€ëŠ¥) 

- ê·¸ë¦¬ê³  ì–˜ê°€ ê³„ì† ìê¸°ê°€ ìš”êµ¬ë¥´íŠ¸ë¥¼ ì´ë¯¸ ë¨¹ì€ê±¸ ê¹Œë¨¹ê±°ë‚˜ 100ê°œëŠ” ë¨¹ì—ˆë‹¤ê³  ì°©ê°í•´ 
  (ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ê¸°ìš¸ê¸° ì†Œì‹¤ê³¼ í­ë°œë¡œ ì¸í•œ ì¥ê¸° ì˜ì¡´ì„±) 

###  So, ì´ ê¸°ìš¸ê¸°ë¥¼ ì¡°ì ˆí•´ë³´ì!! (ì¥ë‹¨ê¸° ê¸°ì–µë§)--> **LSTM&GRU**


**[ì£¼ì˜: ê·¸...í•™ìŠµ ëª©í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸..ë¥¼ ì´í•´í•˜ë ¤ê³  ë°œë²„ë‘¥ì¹˜ë ¤ëŠ” ê³¼ì •ì…ë„¤ë‹¤ğŸ˜œğŸ˜‰ğŸ’œğŸ¤¦â€â™€ï¸. ì‘ì„±ì¤‘]**

- ğŸ† í•™ìŠµ ëª©í‘œ
- Transformerì˜ ì¥ì ê³¼ ì£¼ìš” í”„ë¡œì„¸ìŠ¤ì¸ Self-Attentionì— ëŒ€í•´ ì´í•´í•˜ê³  ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.
- GPT, BERT ê·¸ë¦¬ê³  ë‹¤ë¥¸ ëª¨ë¸ì— ëŒ€í•´ì„œ ê°œëµì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.

**Attention ë©”ì»¤ë‹ˆì¦˜ --(ê·¹ëŒ€í™”)--> íŠ¸ëœìŠ¤í¬ë¨¸ (ê¸°ê³„ë²ˆì—­ new ëª¨ë¸)-(ê¸°ë°˜)---> SOTA(State of the art)(ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸)**

_ì•„ ì ì‹œë§Œ.. ì´ state of the art..? ë§ì´ ë“¤ì–´ë´¤ëŠ”ë°..?_ 
![image](https://user-images.githubusercontent.com/89775352/147628250-77b2a9af-ce4c-4d48-af0e-7449c17fa6c7.png)
10ë§Œ ìœ íŠœë²„ ê°œë°œì ì¹­ê·œê°€..ë‚´ê°€ í¬í•­ì—ì„œ ë°ë ¤ê°„ ë¬¼íšŒì§‘ì—ì„œ ì´ ë“œë¦½ì„ ì³¤ì„ ë•Œ..  ì–´..state-of-the-artëŠ” ìµœì²¨ë‹¨ì´ë¼ëŠ” ëœ» ì•„ë‹ˆì•¼?
ë¼ê³  ë§í•˜ë‹ˆê¹Œ AI ê³µë¶€í•˜ëŠ” ê±”ê°€ ëŒ€ì¶© ì¸ê³µì§€ëŠ¥ ê´€ë ¨í•œê±°ì•¼ ë¼ê³  í–ˆì—ˆë‹¤. [ì˜¤íˆë ¤ ì¢‹ì•„] ì´ì œ ì´í•´í•  ìˆ˜ ìˆëŠ”ê±´ê°€..?

ê°€ë³´ìê·œ!

### **íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)**
RNN ë‹¨ì : ë‹¨ì–´ê°€ Sequential í•˜ë‹¤(ë³‘ë ¬X) -> ê³„ì‚° ì‹œê°„ì´ ê¸¸ì–´ì§„ë‹¤--> íŠ¸ëœìŠ¤í¬ë¨¸ ë‘ë‘¥ (ë‚´ê°€ ì´ê±¸ í•´ê²°í•´ì¤„ê²Œ!!)
ì¦‰, íŠ¸ëœìŠ¤í¬ë¨¸: ë‘ë‘¥ íƒ€ë¼ë¼ë½ ëª¨ë“  í† í° ë‚˜í•œí…Œ ë™ì‹œì— ì¤˜! ë‚˜ëŠ” ë³‘ë ¬ ì—°ì‚° í•  ìˆ˜ ì´ìª„! -> GPUì— ìµœì í™” 
![image](https://user-images.githubusercontent.com/89775352/147629387-23a8bc55-628b-483a-91df-5a887a6141ed.png)
 
![image](https://user-images.githubusercontent.com/89775352/147630547-cdc80de5-2629-4362-8694-383e3bb18ab5.png)

### **Self-Attentionì´ë€**
: íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì£¼ìš” ë©”ì»¤ë‹ˆì¦˜ ì¤‘ í•˜ë‚˜ì„.'
'The animal didn't cross the street because **it** was too tired'  --> ì˜ it ì´ ë­”ì§€ë¥¼ ì•Œì•„ì•¼ ë²ˆì—­ ì œëŒ€ë¡œ í•˜ê² ì§€
ê·¸ë˜ì„œ ìê¸° ìì‹ ì„ í™œìš©í•˜ëŠ” ê±°ì•¼ ì•½ê°„ ê·¸ ë­ëƒ 'ë„¤ ìì‹ ì„ ì•Œë¼','ìê¸° ë³µì œ ë„ë§ˆë±€'

ê·¼ë° **Attentionì´ ë­ëƒ Pay Attention!** 
ë‚´ê°€ êµ¬ê¸€ì— "ì €ëŠ” ì™œ ëˆˆë„ ì½”ë„ ì˜ˆì ê¹Œìš”?" ë¼ê³  ê²€ìƒ‰í•œë‹¤ê³  ì¹˜ì. 
Query: "ì €ëŠ” ì™œ ëˆˆë„ ì½”ë„ ì˜ˆì ê¹Œìš”?" 
Key: ëˆˆ, ì½”, ì˜ˆì ê¹Œìš”
Value: ê´€ë ¨í•œ í˜ì´ì§€ 

**Self-Attentionì€ ê° Query,Key,Value ë²¡í„°ê°€ ëª¨ë‘ ê°€ì¤‘ì¹˜ ë²¡í„°**
ì–´ë–»ê²Œ ì´ ì…€í”„ ì–´í…ì…˜ì„ ì·¨í•´ì£¼ëŠëƒ? ì–´í…ì…˜ì´ë‘ ë¹„ìŠ· 
1.íŠ¹ì • ë‹¨ì–´ì˜ ì¿¼ë¦¬(q) ë²¡í„°ì™€ ëª¨ë“  ë‹¨ì–´ì˜ í‚¤(k) ë²¡í„°ë¥¼ ë‚´ì -> ë‚˜ì˜¤ëŠ” ê°’ì´ Attention ìŠ¤ì½”ì–´(Score)
2.ì´ ê°€ì¤‘ì¹˜ë¥¼ q,k,v ë²¡í„° ì°¨ì›  ì˜ ì œê³±ê·¼ì¸  ë¡œ ë‚˜ëˆ„ì–´ ì¤Œ-> ê³„ì‚° ë³´ì •
3. Softmaxë¥¼ ì·¨í•´ì¤ë‹ˆë‹¤. -> ë‹¨ì–´ì™€ ë¬¸ì¥ ë‚´ ë‹¤ë¥¸ ë‹¨ì–´ê°€ ê°€ì§€ëŠ” ê´€ê³„ì˜ ë¹„ìœ¨ì„ êµ¬í•  ìˆ˜ ìˆìŒ.
4. ë°¸ë¥˜(v) ê° ë‹¨ì–´ì˜ ë²¡í„°ë¥¼ ê³±í•´ì¤€ í›„ ëª¨ë‘ ë”í•¨
![image](https://user-images.githubusercontent.com/89775352/147632810-620a8d74-50df-4e1e-bde3-3495a3c60574.png)

### **Multi-Head Attention**
### **:Self-Attentionì„ ë™ì‹œì— / ë³‘ë ¬ì ìœ¼ë¡œ /ì‹¤í–‰í•˜ëŠ” ê²ƒ**
![image](https://user-images.githubusercontent.com/89775352/147632750-d5454389-602f-4c6e-b9f0-139f3448ac8c.png)

![image](https://user-images.githubusercontent.com/89775352/147632969-ded22629-79d4-4dfc-a173-b04b1b699420.png)

íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ëª¨ë“  sub-layerì—ì„œ ì¶œë ¥ëœ ë²¡í„°ëŠ” Layer normalizationê³¼ Skip connection ê±°ì¹¨ 
ê·¸ ë‹¤ìŒ FFNN(Feed forward neural network) í™œì„±í™” í•¨ìˆ˜(Activation function)ìœ¼ë¡œ ReLUë¥¼ ì‚¬ìš©
### **Masked Self-Attention :ë””ì½”ë” ë¸”ë¡ì—ì„œ ì‚¬ìš©ë˜ëŠ” íŠ¹ìˆ˜í•œ Self-Attention**
![image](https://user-images.githubusercontent.com/89775352/147633175-a62769da-2040-4f77-981d-7242a3aec8f8.png)
